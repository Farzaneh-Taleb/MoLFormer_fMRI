{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-14T10:00:33.745087Z",
     "start_time": "2025-03-14T10:00:30.979366Z"
    }
   },
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer,AutoModelForMaskedLM, AutoModelForCausalLM,AutoModelForSeq2SeqLM\n",
    "import pubchempy as pcp\n",
    "from scipy.io import loadmat\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from NoteBooks.utils import *"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T10:00:33.914168Z",
     "start_time": "2025-03-14T10:00:33.910405Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_representations(tokenizer, model,model_name):\n",
    "\n",
    "    for subject_id in range(1, 4):\n",
    "        CIDs, smiles_subject = read_CIDs(subject_id)\n",
    "        inputs = tokenizer(smiles_subject, padding=True, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs,output_hidden_states=True)\n",
    "        \n",
    "        #save the embeddings as npy together with CID and subject ID and behavior ratings\n",
    "            \n",
    "            for i,output in enumerate(outputs.hidden_states):\n",
    "                np.save(f'results/embeddings_{model_name}_{subject_id}_{i}.npy', output[:,0,:].cpu().numpy())"
   ],
   "id": "d15ab8fabf3601a4",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T10:00:33.924861Z",
     "start_time": "2025-03-14T10:00:33.922562Z"
    }
   },
   "cell_type": "code",
   "source": "base_dir = '../../../T5 EVO'",
   "id": "cf11e43aee3b97d0",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T10:00:33.941547Z",
     "start_time": "2025-03-14T10:00:33.937353Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#get smiles from CID\n",
    "def get_smiles_from_cid(cid):\n",
    "    \n",
    "    compound =  pcp.Compound.from_cid(cid)\n",
    "    return compound.canonical_smiles"
   ],
   "id": "6e26f58f504e0d85",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T10:00:33.954947Z",
     "start_time": "2025-03-14T10:00:33.952937Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "92a0a714f0ec10d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T10:00:34.729440Z",
     "start_time": "2025-03-14T10:00:34.725952Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def read_CIDs(subject_id):\n",
    "    mat1 = loadmat(f'{base_dir}/fmri/Fahime/behavior/behav_ratings_NEMO0{subject_id}.mat')\n",
    "    CIDs = mat1['behav'][0][0]['cid']\n",
    "    CIDs = CIDs.squeeze(1)\n",
    "    CIDs = CIDs.tolist()\n",
    "    smiles_subject = []\n",
    "    for cid in CIDs:\n",
    "        smiles = get_smiles_from_cid(cid)\n",
    "        smiles_subject.append(smiles)\n",
    "    return CIDs, smiles_subject"
   ],
   "id": "3ba1b53f1c8b63bd",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T09:51:57.440254Z",
     "start_time": "2025-03-14T09:50:56.816645Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for subject_id in range(1, 4):\n",
    "    CIDs, smiles_subject = read_CIDs(subject_id)\n",
    "    #save the CIDs and smiles\n",
    "    pd.DataFrame({'CIDs':CIDs, 'smiles':smiles_subject}).to_csv(f'results/CIDs_smiles_{subject_id}.csv')"
   ],
   "id": "281357e25edab603",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m subject_id \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m4\u001B[39m):\n\u001B[0;32m----> 2\u001B[0m     CIDs, smiles_subject \u001B[38;5;241m=\u001B[39m \u001B[43mread_CIDs\u001B[49m\u001B[43m(\u001B[49m\u001B[43msubject_id\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;66;03m#save the CIDs and smiles\u001B[39;00m\n\u001B[1;32m      4\u001B[0m     pd\u001B[38;5;241m.\u001B[39mDataFrame({\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCIDs\u001B[39m\u001B[38;5;124m'\u001B[39m:CIDs, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msmiles\u001B[39m\u001B[38;5;124m'\u001B[39m:smiles_subject})\u001B[38;5;241m.\u001B[39mto_csv(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mresults/CIDs_smiles_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00msubject_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[0;32mIn[5], line 8\u001B[0m, in \u001B[0;36mread_CIDs\u001B[0;34m(subject_id)\u001B[0m\n\u001B[1;32m      6\u001B[0m smiles_subject \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m cid \u001B[38;5;129;01min\u001B[39;00m CIDs:\n\u001B[0;32m----> 8\u001B[0m     smiles \u001B[38;5;241m=\u001B[39m \u001B[43mget_smiles_from_cid\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcid\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m     smiles_subject\u001B[38;5;241m.\u001B[39mappend(smiles)\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m CIDs, smiles_subject\n",
      "Cell \u001B[0;32mIn[4], line 4\u001B[0m, in \u001B[0;36mget_smiles_from_cid\u001B[0;34m(cid)\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_smiles_from_cid\u001B[39m(cid):\n\u001B[0;32m----> 4\u001B[0m     compound \u001B[38;5;241m=\u001B[39m  \u001B[43mpcp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mCompound\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_cid\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcid\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m compound\u001B[38;5;241m.\u001B[39mcanonical_smiles\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/MoLFormer_fMRI/lib/python3.11/site-packages/pubchempy.py:726\u001B[0m, in \u001B[0;36mCompound.from_cid\u001B[0;34m(cls, cid, **kwargs)\u001B[0m\n\u001B[1;32m    716\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[1;32m    717\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfrom_cid\u001B[39m(\u001B[38;5;28mcls\u001B[39m, cid, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    718\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Retrieve the Compound record for the specified CID.\u001B[39;00m\n\u001B[1;32m    719\u001B[0m \n\u001B[1;32m    720\u001B[0m \u001B[38;5;124;03m    Usage::\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    724\u001B[0m \u001B[38;5;124;03m    :param int cid: The PubChem Compound Identifier (CID).\u001B[39;00m\n\u001B[1;32m    725\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 726\u001B[0m     record \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mloads(\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcid\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mread()\u001B[38;5;241m.\u001B[39mdecode())[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPC_Compounds\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    727\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mcls\u001B[39m(record)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/MoLFormer_fMRI/lib/python3.11/site-packages/pubchempy.py:271\u001B[0m, in \u001B[0;36mrequest\u001B[0;34m(identifier, namespace, domain, operation, output, searchtype, **kwargs)\u001B[0m\n\u001B[1;32m    269\u001B[0m     log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mRequest URL: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m, apiurl)\n\u001B[1;32m    270\u001B[0m     log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mRequest data: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m, postdata)\n\u001B[0;32m--> 271\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43murlopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mapiurl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpostdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    272\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m response\n\u001B[1;32m    273\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m HTTPError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/MoLFormer_fMRI/lib/python3.11/urllib/request.py:216\u001B[0m, in \u001B[0;36murlopen\u001B[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001B[0m\n\u001B[1;32m    214\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    215\u001B[0m     opener \u001B[38;5;241m=\u001B[39m _opener\n\u001B[0;32m--> 216\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mopener\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/MoLFormer_fMRI/lib/python3.11/urllib/request.py:519\u001B[0m, in \u001B[0;36mOpenerDirector.open\u001B[0;34m(self, fullurl, data, timeout)\u001B[0m\n\u001B[1;32m    516\u001B[0m     req \u001B[38;5;241m=\u001B[39m meth(req)\n\u001B[1;32m    518\u001B[0m sys\u001B[38;5;241m.\u001B[39maudit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124murllib.Request\u001B[39m\u001B[38;5;124m'\u001B[39m, req\u001B[38;5;241m.\u001B[39mfull_url, req\u001B[38;5;241m.\u001B[39mdata, req\u001B[38;5;241m.\u001B[39mheaders, req\u001B[38;5;241m.\u001B[39mget_method())\n\u001B[0;32m--> 519\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    521\u001B[0m \u001B[38;5;66;03m# post-process response\u001B[39;00m\n\u001B[1;32m    522\u001B[0m meth_name \u001B[38;5;241m=\u001B[39m protocol\u001B[38;5;241m+\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_response\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/MoLFormer_fMRI/lib/python3.11/urllib/request.py:536\u001B[0m, in \u001B[0;36mOpenerDirector._open\u001B[0;34m(self, req, data)\u001B[0m\n\u001B[1;32m    533\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n\u001B[1;32m    535\u001B[0m protocol \u001B[38;5;241m=\u001B[39m req\u001B[38;5;241m.\u001B[39mtype\n\u001B[0;32m--> 536\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_chain\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle_open\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprotocol\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprotocol\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\n\u001B[1;32m    537\u001B[0m \u001B[43m                          \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m_open\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreq\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m result:\n\u001B[1;32m    539\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/MoLFormer_fMRI/lib/python3.11/urllib/request.py:496\u001B[0m, in \u001B[0;36mOpenerDirector._call_chain\u001B[0;34m(self, chain, kind, meth_name, *args)\u001B[0m\n\u001B[1;32m    494\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m handler \u001B[38;5;129;01min\u001B[39;00m handlers:\n\u001B[1;32m    495\u001B[0m     func \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(handler, meth_name)\n\u001B[0;32m--> 496\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    497\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    498\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/MoLFormer_fMRI/lib/python3.11/urllib/request.py:1391\u001B[0m, in \u001B[0;36mHTTPSHandler.https_open\u001B[0;34m(self, req)\u001B[0m\n\u001B[1;32m   1390\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mhttps_open\u001B[39m(\u001B[38;5;28mself\u001B[39m, req):\n\u001B[0;32m-> 1391\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdo_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhttp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mHTTPSConnection\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreq\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1392\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcontext\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_context\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcheck_hostname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check_hostname\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/MoLFormer_fMRI/lib/python3.11/urllib/request.py:1348\u001B[0m, in \u001B[0;36mAbstractHTTPHandler.do_open\u001B[0;34m(self, http_class, req, **http_conn_args)\u001B[0m\n\u001B[1;32m   1346\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1347\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1348\u001B[0m         \u001B[43mh\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselector\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1349\u001B[0m \u001B[43m                  \u001B[49m\u001B[43mencode_chunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhas_header\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mTransfer-encoding\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err: \u001B[38;5;66;03m# timeout error\u001B[39;00m\n\u001B[1;32m   1351\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m URLError(err)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/MoLFormer_fMRI/lib/python3.11/http/client.py:1303\u001B[0m, in \u001B[0;36mHTTPConnection.request\u001B[0;34m(self, method, url, body, headers, encode_chunked)\u001B[0m\n\u001B[1;32m   1300\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrequest\u001B[39m(\u001B[38;5;28mself\u001B[39m, method, url, body\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, headers\u001B[38;5;241m=\u001B[39m{}, \u001B[38;5;241m*\u001B[39m,\n\u001B[1;32m   1301\u001B[0m             encode_chunked\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m   1302\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001B[39;00m\n\u001B[0;32m-> 1303\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_send_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencode_chunked\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/MoLFormer_fMRI/lib/python3.11/http/client.py:1349\u001B[0m, in \u001B[0;36mHTTPConnection._send_request\u001B[0;34m(self, method, url, body, headers, encode_chunked)\u001B[0m\n\u001B[1;32m   1345\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(body, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m   1346\u001B[0m     \u001B[38;5;66;03m# RFC 2616 Section 3.7.1 says that text default has a\u001B[39;00m\n\u001B[1;32m   1347\u001B[0m     \u001B[38;5;66;03m# default charset of iso-8859-1.\u001B[39;00m\n\u001B[1;32m   1348\u001B[0m     body \u001B[38;5;241m=\u001B[39m _encode(body, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbody\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m-> 1349\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mendheaders\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencode_chunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencode_chunked\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/MoLFormer_fMRI/lib/python3.11/http/client.py:1298\u001B[0m, in \u001B[0;36mHTTPConnection.endheaders\u001B[0;34m(self, message_body, encode_chunked)\u001B[0m\n\u001B[1;32m   1296\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1297\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m CannotSendHeader()\n\u001B[0;32m-> 1298\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_send_output\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmessage_body\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencode_chunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencode_chunked\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/MoLFormer_fMRI/lib/python3.11/http/client.py:1058\u001B[0m, in \u001B[0;36mHTTPConnection._send_output\u001B[0;34m(self, message_body, encode_chunked)\u001B[0m\n\u001B[1;32m   1056\u001B[0m msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\r\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_buffer)\n\u001B[1;32m   1057\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_buffer[:]\n\u001B[0;32m-> 1058\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmsg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1060\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m message_body \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1061\u001B[0m \n\u001B[1;32m   1062\u001B[0m     \u001B[38;5;66;03m# create a consistent interface to message_body\u001B[39;00m\n\u001B[1;32m   1063\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(message_body, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mread\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m   1064\u001B[0m         \u001B[38;5;66;03m# Let file-like take precedence over byte-like.  This\u001B[39;00m\n\u001B[1;32m   1065\u001B[0m         \u001B[38;5;66;03m# is needed to allow the current position of mmap'ed\u001B[39;00m\n\u001B[1;32m   1066\u001B[0m         \u001B[38;5;66;03m# files to be taken into account.\u001B[39;00m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/MoLFormer_fMRI/lib/python3.11/http/client.py:996\u001B[0m, in \u001B[0;36mHTTPConnection.send\u001B[0;34m(self, data)\u001B[0m\n\u001B[1;32m    994\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msock \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    995\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_open:\n\u001B[0;32m--> 996\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    997\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    998\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m NotConnected()\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/MoLFormer_fMRI/lib/python3.11/http/client.py:1468\u001B[0m, in \u001B[0;36mHTTPSConnection.connect\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1465\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mconnect\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m   1466\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConnect to a host on a given (SSL) port.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m-> 1468\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1470\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tunnel_host:\n\u001B[1;32m   1471\u001B[0m         server_hostname \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tunnel_host\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/MoLFormer_fMRI/lib/python3.11/http/client.py:962\u001B[0m, in \u001B[0;36mHTTPConnection.connect\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    960\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Connect to the host and port specified in __init__.\"\"\"\u001B[39;00m\n\u001B[1;32m    961\u001B[0m sys\u001B[38;5;241m.\u001B[39maudit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttp.client.connect\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mself\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhost, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mport)\n\u001B[0;32m--> 962\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msock \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_connection\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    963\u001B[0m \u001B[43m    \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhost\u001B[49m\u001B[43m,\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mport\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msource_address\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    964\u001B[0m \u001B[38;5;66;03m# Might fail in OSs that don't implement TCP_NODELAY\u001B[39;00m\n\u001B[1;32m    965\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/MoLFormer_fMRI/lib/python3.11/socket.py:848\u001B[0m, in \u001B[0;36mcreate_connection\u001B[0;34m(address, timeout, source_address, all_errors)\u001B[0m\n\u001B[1;32m    846\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m source_address:\n\u001B[1;32m    847\u001B[0m     sock\u001B[38;5;241m.\u001B[39mbind(source_address)\n\u001B[0;32m--> 848\u001B[0m \u001B[43msock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43msa\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    849\u001B[0m \u001B[38;5;66;03m# Break explicitly a reference cycle\u001B[39;00m\n\u001B[1;32m    850\u001B[0m exceptions\u001B[38;5;241m.\u001B[39mclear()\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T09:50:11.844437Z",
     "start_time": "2025-01-07T15:02:47.251129Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for subject_id in range(1, 4):\n",
    "    mat1 = loadmat(f'{base_dir}/fmri/Fahime/behavior/behav_ratings_NEMO0{subject_id}.mat')\n",
    "    ratings = mat1['behav'][0][0]['ratings']\n",
    "    print(ratings.shape)\n",
    "    #save the ratings\n",
    "    np.save(f'results/ratings_{subject_id}.npy', ratings)"
   ],
   "id": "fbedbb67cf9786a3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160, 18)\n",
      "(160, 18)\n",
      "(160, 18)\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# OpenPOM",
   "id": "bcd066555a937784"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T10:07:03.907516Z",
     "start_time": "2025-03-14T10:03:45.797655Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# model = AutoModel.from_pretrained(\"ibm/MoLFormer-XL-both-10pct\", deterministic_eval=True, trust_remote_code=True)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"ibm/MoLFormer-XL-both-10pct\", trust_remote_code=True)\n",
    "all_outputs = []\n",
    "layers = []\n",
    "all_subjects = []\n",
    "for subject_id in range(1, 4):\n",
    "    CIDs, smiles_subject = read_CIDs(subject_id)\n",
    "    with torch.no_grad():\n",
    "        outputs = read_pom(base_dir, CIDs)\n",
    "        print(outputs.shape)\n",
    "            #save to npy\n",
    "        np.save(f'results/openpom_{subject_id}_1000.npy', outputs)\n",
    "        # save the output and layers\n"
   ],
   "id": "a0a615a1c9357d08",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160, 256)\n",
      "(160, 256)\n",
      "(160, 256)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# MoLFormer-XL-both-10pct",
   "id": "659c357405615704"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T21:03:57.047530Z",
     "start_time": "2025-01-06T21:00:03.803379Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = AutoModel.from_pretrained(\"ibm/MoLFormer-XL-both-10pct\", deterministic_eval=True, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ibm/MoLFormer-XL-both-10pct\", trust_remote_code=True)\n",
    "all_outputs = []\n",
    "layers = []\n",
    "all_subjects = []\n",
    "for subject_id in range(1, 4):\n",
    "    CIDs, smiles_subject = read_CIDs(subject_id)\n",
    "    inputs = tokenizer(smiles_subject, padding=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs,output_hidden_states=True)\n",
    "        for i,output in enumerate(outputs.hidden_states):\n",
    "            #save to npy\n",
    "            np.save(f'results/embeddings_MoLFormer-XL-both-10pct_{subject_id}_{i}.npy', output[:,0,:].cpu().numpy())\n",
    "        # save the output and layers\n",
    "   "
   ],
   "id": "9682defb4c9030c5",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T21:04:29.632390Z",
     "start_time": "2025-01-06T21:04:29.618895Z"
    }
   },
   "cell_type": "code",
   "source": "np.load('results/embeddings_MoLFormer-XL-both-10pct_1_0.npy').shape",
   "id": "77c8943f1fa307f9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160, 768)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ChemBERTa-zinc-base-v1",
   "id": "aa53faca498e649c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T21:14:56.602147Z",
     "start_time": "2025-01-06T21:10:54.423010Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load model directly\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "extract_representations(tokenizer, model,'embeddings_ChemBERTa-zinc-base-v1')"
   ],
   "id": "2348c663042dd61b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Some weights of the model checkpoint at seyonec/ChemBERTa-zinc-base-v1 were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# MoLGPT",
   "id": "37b090f104102a83"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T13:08:35.822332Z",
     "start_time": "2025-01-09T13:04:51.943369Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"msb-roshan/molgpt\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"msb-roshan/molgpt\")\n",
    "extract_representations(tokenizer, model,'molgpt')"
   ],
   "id": "70aa77643ac6c5a1",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# MoLGen",
   "id": "246c6f93f37cb03b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T12:57:50.516668Z",
     "start_time": "2025-01-09T12:51:43.842355Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"zjunlp/MolGen-large\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"zjunlp/MolGen-large\")\n",
    "extract_representations(tokenizer, model,'MolGen-large')"
   ],
   "id": "9cd7ad386bac15df",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Seq2SeqLMOutput' object has no attribute 'hidden_states'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m AutoTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mzjunlp/MolGen-large\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      2\u001B[0m model \u001B[38;5;241m=\u001B[39m AutoModelForSeq2SeqLM\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mzjunlp/MolGen-large\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 3\u001B[0m \u001B[43mextract_representations\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mMolGen-large\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[2], line 11\u001B[0m, in \u001B[0;36mextract_representations\u001B[0;34m(tokenizer, model, model_name)\u001B[0m\n\u001B[1;32m      7\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m model(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39minputs,output_hidden_states\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m#save the embeddings as npy together with CID and subject ID and behavior ratings\u001B[39;00m\n\u001B[0;32m---> 11\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i,output \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[43moutputs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhidden_states\u001B[49m):\n\u001B[1;32m     12\u001B[0m         np\u001B[38;5;241m.\u001B[39msave(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mresults/embeddings_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00msubject_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.npy\u001B[39m\u001B[38;5;124m'\u001B[39m, output[:,\u001B[38;5;241m0\u001B[39m,:]\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mnumpy())\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'Seq2SeqLMOutput' object has no attribute 'hidden_states'"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# BASRTSmiles",
   "id": "a1e0aa2d1cc61d01"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T13:04:03.853198Z",
     "start_time": "2025-01-09T12:58:48.163142Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gayane/BARTSmiles\", add_prefix_space=True)\n",
    "model = AutoModel.from_pretrained('gayane/BARTSmiles')\n",
    "extract_representations(tokenizer, model,'BARTSmiles')"
   ],
   "id": "b605cb7202af5074",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m AutoTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgayane/BARTSmiles\u001B[39m\u001B[38;5;124m\"\u001B[39m, add_prefix_space\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m      2\u001B[0m model \u001B[38;5;241m=\u001B[39m AutoModel\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgayane/BARTSmiles\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m----> 3\u001B[0m \u001B[43mextract_representations\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mBARTSmiles\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[2], line 5\u001B[0m, in \u001B[0;36mextract_representations\u001B[0;34m(tokenizer, model, model_name)\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m subject_id \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m4\u001B[39m):\n\u001B[1;32m      4\u001B[0m     CIDs, smiles_subject \u001B[38;5;241m=\u001B[39m read_CIDs(subject_id)\n\u001B[0;32m----> 5\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m \u001B[43mtokenizer\u001B[49m\u001B[43m(\u001B[49m\u001B[43msmiles_subject\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpt\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m      7\u001B[0m         outputs \u001B[38;5;241m=\u001B[39m model(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39minputs,output_hidden_states\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/MoLFormer_fMRI/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2860\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.__call__\u001B[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001B[0m\n\u001B[1;32m   2858\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_in_target_context_manager:\n\u001B[1;32m   2859\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_switch_to_input_mode()\n\u001B[0;32m-> 2860\u001B[0m     encodings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_one\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtext_pair\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtext_pair\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mall_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2861\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m text_target \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   2862\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_switch_to_target_mode()\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/MoLFormer_fMRI/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2948\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase._call_one\u001B[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001B[0m\n\u001B[1;32m   2943\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   2944\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbatch length of `text`: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(text)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m does not match batch length of `text_pair`:\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2945\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(text_pair)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2946\u001B[0m         )\n\u001B[1;32m   2947\u001B[0m     batch_text_or_text_pairs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(text, text_pair)) \u001B[38;5;28;01mif\u001B[39;00m text_pair \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m text\n\u001B[0;32m-> 2948\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_encode_plus\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2949\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbatch_text_or_text_pairs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_text_or_text_pairs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2950\u001B[0m \u001B[43m        \u001B[49m\u001B[43madd_special_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43madd_special_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2951\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpadding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2952\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtruncation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtruncation\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2953\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2954\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstride\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2955\u001B[0m \u001B[43m        \u001B[49m\u001B[43mis_split_into_words\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_split_into_words\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2956\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2957\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpadding_side\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpadding_side\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2958\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_tensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2959\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_token_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_token_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2960\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2961\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_overflowing_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_overflowing_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2962\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_special_tokens_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_special_tokens_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2963\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_offsets_mapping\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_offsets_mapping\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2964\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2965\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2966\u001B[0m \u001B[43m        \u001B[49m\u001B[43msplit_special_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msplit_special_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2967\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2968\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2969\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   2970\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencode_plus(\n\u001B[1;32m   2971\u001B[0m         text\u001B[38;5;241m=\u001B[39mtext,\n\u001B[1;32m   2972\u001B[0m         text_pair\u001B[38;5;241m=\u001B[39mtext_pair,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2990\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   2991\u001B[0m     )\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/MoLFormer_fMRI/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3141\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001B[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001B[0m\n\u001B[1;32m   3124\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   3125\u001B[0m \u001B[38;5;124;03mTokenize and prepare for the model a list of sequences or a list of pairs of sequences.\u001B[39;00m\n\u001B[1;32m   3126\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3137\u001B[0m \u001B[38;5;124;03m        details in `encode_plus`).\u001B[39;00m\n\u001B[1;32m   3138\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   3140\u001B[0m \u001B[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001B[39;00m\n\u001B[0;32m-> 3141\u001B[0m padding_strategy, truncation_strategy, max_length, kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_padding_truncation_strategies\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   3142\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpadding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3143\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtruncation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtruncation\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3144\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3145\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3146\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3147\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3148\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3150\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_batch_encode_plus(\n\u001B[1;32m   3151\u001B[0m     batch_text_or_text_pairs\u001B[38;5;241m=\u001B[39mbatch_text_or_text_pairs,\n\u001B[1;32m   3152\u001B[0m     add_special_tokens\u001B[38;5;241m=\u001B[39madd_special_tokens,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3169\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   3170\u001B[0m )\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/MoLFormer_fMRI/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2762\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase._get_padding_truncation_strategies\u001B[0;34m(self, padding, truncation, max_length, pad_to_multiple_of, verbose, **kwargs)\u001B[0m\n\u001B[1;32m   2760\u001B[0m \u001B[38;5;66;03m# Test if we have a padding token\u001B[39;00m\n\u001B[1;32m   2761\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m padding_strategy \u001B[38;5;241m!=\u001B[39m PaddingStrategy\u001B[38;5;241m.\u001B[39mDO_NOT_PAD \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpad_token \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpad_token_id \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0\u001B[39m):\n\u001B[0;32m-> 2762\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   2763\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAsking to pad but the tokenizer does not have a padding token. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2764\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2765\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mor add a new pad token via `tokenizer.add_special_tokens(\u001B[39m\u001B[38;5;124m{\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpad_token\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m: \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m[PAD]\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m})`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2766\u001B[0m     )\n\u001B[1;32m   2768\u001B[0m \u001B[38;5;66;03m# Check that we will truncate to a multiple of pad_to_multiple_of if both are provided\u001B[39;00m\n\u001B[1;32m   2769\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   2770\u001B[0m     truncation_strategy \u001B[38;5;241m!=\u001B[39m TruncationStrategy\u001B[38;5;241m.\u001B[39mDO_NOT_TRUNCATE\n\u001B[1;32m   2771\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m padding_strategy \u001B[38;5;241m!=\u001B[39m PaddingStrategy\u001B[38;5;241m.\u001B[39mDO_NOT_PAD\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2774\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (max_length \u001B[38;5;241m%\u001B[39m pad_to_multiple_of \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m   2775\u001B[0m ):\n",
      "\u001B[0;31mValueError\u001B[0m: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`."
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#ChemGPT",
   "id": "7b7701cb469fba8b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T12:43:57.064905Z",
     "start_time": "2025-01-09T12:42:33.980129Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"ncfrey/ChemGPT-4.7M\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"ncfrey/ChemGPT-4.7M\")\n",
    "extract_representations(tokenizer, model,'ChemGPT-4_7M')"
   ],
   "id": "63c628d7146759e8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m AutoTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mncfrey/ChemGPT-4.7M\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      2\u001B[0m model \u001B[38;5;241m=\u001B[39m AutoModelForCausalLM\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mncfrey/ChemGPT-4.7M\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 3\u001B[0m \u001B[43mextract_representations\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mChemGPT-4_7M\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[2], line 5\u001B[0m, in \u001B[0;36mextract_representations\u001B[0;34m(tokenizer, model, model_name)\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m subject_id \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m4\u001B[39m):\n\u001B[1;32m      4\u001B[0m     CIDs, smiles_subject \u001B[38;5;241m=\u001B[39m read_CIDs(subject_id)\n\u001B[0;32m----> 5\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m \u001B[43mtokenizer\u001B[49m\u001B[43m(\u001B[49m\u001B[43msmiles_subject\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpt\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m      7\u001B[0m         outputs \u001B[38;5;241m=\u001B[39m model(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39minputs,output_hidden_states\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/MoLFormer_fMRI/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2860\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.__call__\u001B[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001B[0m\n\u001B[1;32m   2858\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_in_target_context_manager:\n\u001B[1;32m   2859\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_switch_to_input_mode()\n\u001B[0;32m-> 2860\u001B[0m     encodings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_one\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtext_pair\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtext_pair\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mall_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2861\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m text_target \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   2862\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_switch_to_target_mode()\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/MoLFormer_fMRI/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2948\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase._call_one\u001B[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001B[0m\n\u001B[1;32m   2943\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   2944\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbatch length of `text`: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(text)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m does not match batch length of `text_pair`:\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2945\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(text_pair)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2946\u001B[0m         )\n\u001B[1;32m   2947\u001B[0m     batch_text_or_text_pairs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(text, text_pair)) \u001B[38;5;28;01mif\u001B[39;00m text_pair \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m text\n\u001B[0;32m-> 2948\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_encode_plus\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2949\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbatch_text_or_text_pairs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_text_or_text_pairs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2950\u001B[0m \u001B[43m        \u001B[49m\u001B[43madd_special_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43madd_special_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2951\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpadding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2952\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtruncation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtruncation\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2953\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2954\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstride\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2955\u001B[0m \u001B[43m        \u001B[49m\u001B[43mis_split_into_words\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_split_into_words\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2956\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2957\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpadding_side\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpadding_side\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2958\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_tensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2959\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_token_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_token_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2960\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2961\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_overflowing_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_overflowing_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2962\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_special_tokens_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_special_tokens_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2963\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_offsets_mapping\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_offsets_mapping\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2964\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2965\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2966\u001B[0m \u001B[43m        \u001B[49m\u001B[43msplit_special_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msplit_special_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2967\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2968\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2969\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   2970\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencode_plus(\n\u001B[1;32m   2971\u001B[0m         text\u001B[38;5;241m=\u001B[39mtext,\n\u001B[1;32m   2972\u001B[0m         text_pair\u001B[38;5;241m=\u001B[39mtext_pair,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2990\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   2991\u001B[0m     )\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/MoLFormer_fMRI/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3141\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001B[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001B[0m\n\u001B[1;32m   3124\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   3125\u001B[0m \u001B[38;5;124;03mTokenize and prepare for the model a list of sequences or a list of pairs of sequences.\u001B[39;00m\n\u001B[1;32m   3126\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3137\u001B[0m \u001B[38;5;124;03m        details in `encode_plus`).\u001B[39;00m\n\u001B[1;32m   3138\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   3140\u001B[0m \u001B[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001B[39;00m\n\u001B[0;32m-> 3141\u001B[0m padding_strategy, truncation_strategy, max_length, kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_padding_truncation_strategies\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   3142\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpadding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3143\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtruncation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtruncation\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3144\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3145\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3146\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3147\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3148\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3150\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_batch_encode_plus(\n\u001B[1;32m   3151\u001B[0m     batch_text_or_text_pairs\u001B[38;5;241m=\u001B[39mbatch_text_or_text_pairs,\n\u001B[1;32m   3152\u001B[0m     add_special_tokens\u001B[38;5;241m=\u001B[39madd_special_tokens,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3169\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   3170\u001B[0m )\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/MoLFormer_fMRI/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2762\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase._get_padding_truncation_strategies\u001B[0;34m(self, padding, truncation, max_length, pad_to_multiple_of, verbose, **kwargs)\u001B[0m\n\u001B[1;32m   2760\u001B[0m \u001B[38;5;66;03m# Test if we have a padding token\u001B[39;00m\n\u001B[1;32m   2761\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m padding_strategy \u001B[38;5;241m!=\u001B[39m PaddingStrategy\u001B[38;5;241m.\u001B[39mDO_NOT_PAD \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpad_token \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpad_token_id \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0\u001B[39m):\n\u001B[0;32m-> 2762\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   2763\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAsking to pad but the tokenizer does not have a padding token. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2764\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2765\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mor add a new pad token via `tokenizer.add_special_tokens(\u001B[39m\u001B[38;5;124m{\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpad_token\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m: \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m[PAD]\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m})`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2766\u001B[0m     )\n\u001B[1;32m   2768\u001B[0m \u001B[38;5;66;03m# Check that we will truncate to a multiple of pad_to_multiple_of if both are provided\u001B[39;00m\n\u001B[1;32m   2769\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   2770\u001B[0m     truncation_strategy \u001B[38;5;241m!=\u001B[39m TruncationStrategy\u001B[38;5;241m.\u001B[39mDO_NOT_TRUNCATE\n\u001B[1;32m   2771\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m padding_strategy \u001B[38;5;241m!=\u001B[39m PaddingStrategy\u001B[38;5;241m.\u001B[39mDO_NOT_PAD\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2774\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (max_length \u001B[38;5;241m%\u001B[39m pad_to_multiple_of \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m   2775\u001B[0m ):\n",
      "\u001B[0;31mValueError\u001B[0m: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`."
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Selformer",
   "id": "a01c83f720303882"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T12:40:29.362714Z",
     "start_time": "2025-01-09T12:36:45.936238Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"HUBioDataLab/SELFormer\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"HUBioDataLab/SELFormer\")\n",
    "extract_representations(tokenizer, model,'SELFormer')"
   ],
   "id": "c0cfa6a374f032b4",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4588d9479c6937ab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b81afa2a271c7747"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b91a7d438df2fb30"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# IBM SmallMoleculeMultiViewModel",
   "id": "b42ec3f4abee951d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Necessary imports\n",
    "from bmfm_sm.api.smmv_api import SmallMoleculeMultiViewModel\n",
    "from bmfm_sm.core.data_modules.namespace import LateFusionStrategy\n",
    "\n",
    "# Load Model\n",
    "model = SmallMoleculeMultiViewModel.from_pretrained(\n",
    "    LateFusionStrategy.ATTENTIONAL,\n",
    "    model_path=\"ibm/biomed.sm.mv-te-84m\",\n",
    "    huggingface=True\n",
    ")\n",
    "\n",
    "# Load Model and get embeddings for a molecule\n",
    "example_smiles = \"CC(C)CC1=CC=C(C=C1)C(C)C(=O)O\"\n",
    "example_emb = SmallMoleculeMultiViewModel.get_embeddings(\n",
    "    smiles=example_smiles,\n",
    "    model_path=\"ibm/biomed.sm.mv-te-84m\",\n",
    "    huggingface=True,\n",
    ")\n",
    "print(example_emb.shape)"
   ],
   "id": "a6c48de48d07f304"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Necessary imports\n",
    "from bmfm_sm.api.smmv_api import SmallMoleculeMultiViewModel\n",
    "from bmfm_sm.core.data_modules.namespace import LateFusionStrategy\n",
    "\n",
    "# Load Model\n",
    "model = SmallMoleculeMultiViewModel.from_pretrained(\n",
    "    LateFusionStrategy.ATTENTIONAL,\n",
    "    model_path=\"ibm/biomed.sm.mv-te-84m\",\n",
    "    huggingface=True\n",
    ")\n",
    "\n",
    "# Load Model and get embeddings for a molecule\n",
    "example_smiles = \"CC(C)CC1=CC=C(C=C1)C(C)C(=O)O\"\n",
    "example_emb = SmallMoleculeMultiViewModel.get_embeddings(\n",
    "    smiles=example_smiles,\n",
    "    model_path=\"ibm/biomed.sm.mv-te-84m\",\n",
    "    huggingface=True,\n",
    ")\n",
    "print(example_emb.shape)"
   ],
   "id": "fa1220819e34c689"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ChemBERT_ChEMBL_pretrained",
   "id": "817d7903fd4269d7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T22:33:43.059543Z",
     "start_time": "2025-01-06T22:29:57.363657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"jonghyunlee/ChemBERT_ChEMBL_pretrained\")\n",
    "model = AutoModel.from_pretrained(\"jonghyunlee/ChemBERT_ChEMBL_pretrained\")\n",
    "all_outputs = []\n",
    "layers = []\n",
    "all_subjects = []\n",
    "for subject_id in range(1, 4):\n",
    "    CIDs, smiles_subject = read_CIDs(subject_id)\n",
    "    inputs = tokenizer(smiles_subject, padding=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs,output_hidden_states=True)\n",
    "    \n",
    "    #save the embeddings as npy together with CID and subject ID and behavior ratings\n",
    "        \n",
    "        for i,output in enumerate(outputs.hidden_states):\n",
    "            np.save(f'results/embeddings_ChemBERT_ChEMBL_pretrained_{subject_id}_{i}.npy', output[:,0,:].cpu().numpy())"
   ],
   "id": "f9a04b5f5895f807",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Some weights of BertModel were not initialized from the model checkpoint at jonghyunlee/ChemBERT_ChEMBL_pretrained and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# GTMGC",
   "id": "4162ec01ff0121bf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = MoleBERTTokenizer.from_pretrained(\"RichXuOvO/MoleBERT-Tokenizer\")\n",
    "model = GTMGCForGraphRegression.from_pretrained(\"RichXuOvO/GTMGC_Small-Molecule3D-Random-Gap\")  \n",
    "extract_representations(tokenizer, model,'GTMGC_Small-Molecule3D-Random-Gap')"
   ],
   "id": "784df9bb270eabac"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MoLFormer_fMRI",
   "language": "python",
   "name": "molformer_fmri"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
